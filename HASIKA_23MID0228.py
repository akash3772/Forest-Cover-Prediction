# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g1-oM42xbTmDRYncX0Rx2VTiegK08nty
"""



"""# Task
Analyze the image dataset provided in the file "/content/archive (8).zip", train a ResNet50 model on the data, and output the results.

## Extract data

### Subtask:
Extract the contents of the uploaded zip file.

**Reasoning**:
The first step is to extract the contents of the zip file using the `zipfile` module.
"""

import zipfile
import os

zip_file_path = "/content/archive (8).zip"
extracted_dir = "extracted_data"

# Create the extraction directory if it doesn't exist
os.makedirs(extracted_dir, exist_ok=True)

with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extracted_dir)

print(f"Successfully extracted contents to {extracted_dir}")

"""## Load and analyze data

### Subtask:
Load the images and their labels from the extracted data and analyze the dataset, including visualizing some images and checking the class distribution.

**Reasoning**:
The goal is to load and analyze the image dataset. The first steps involve setting up the environment for loading the dataset, loading the data, and inspecting its basic structure, including class names. These steps can be grouped into a single code block.
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import os

# 1. Define the root directory
extracted_dir = "extracted_data"
root_dir = os.path.join(extracted_dir, os.listdir(extracted_dir)[0]) # Assuming the zip contains a single main directory

# 2. Load the image dataset
img_height = 224
img_width = 224
batch_size = 32

train_ds = tf.keras.utils.image_dataset_from_directory(
    root_dir,
    validation_split=0.2,
    subset="training",
    seed=123,
    image_size=(img_height, img_width),
    batch_size=batch_size)

val_ds = tf.keras.utils.image_dataset_from_directory(
    root_dir,
    validation_split=0.2,
    subset="validation",
    seed=123,
    image_size=(img_height, img_width),
    batch_size=batch_size)


# 3. Get the class names
class_names = train_ds.class_names

# 4. Print the class names and the number of classes
print(f"Class names: {class_names}")
print(f"Number of classes: {len(class_names)}")

"""**Reasoning**:
The previous attempt to load the dataset failed because the `root_dir` was set incorrectly. The error message indicates that the code was looking for a directory named `amazon.csv` inside `extracted_data`. This happened because `os.listdir(extracted_dir)[0]` returned "amazon.csv" which is likely a file and not the root directory of the images. I need to correct the `root_dir` to point to the actual directory containing the class subdirectories within the extracted data. I will also include the visualization and class distribution calculation steps in this code block as they are related to analyzing the loaded dataset.


"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import os

# 1. Define the root directory
extracted_dir = "extracted_data"
# Assuming the extracted directory contains a single main folder which contains the image classes
# Let's find the actual image directory by looking for subdirectories within extracted_data
image_dir = None
for item in os.listdir(extracted_dir):
    item_path = os.path.join(extracted_dir, item)
    if os.path.isdir(item_path):
        # Check if this directory contains subdirectories (classes)
        if any(os.path.isdir(os.path.join(item_path, sub_item)) for sub_item in os.listdir(item_path)):
            image_dir = item_path
            break

if image_dir is None:
    raise FileNotFoundError("Could not find the main image directory within extracted_data")

root_dir = image_dir


# 2. Use image_dataset_from_directory to load the image dataset
img_height = 224
img_width = 224
batch_size = 32

train_ds = tf.keras.utils.image_dataset_from_directory(
    root_dir,
    validation_split=0.2,
    subset="training",
    seed=123,
    image_size=(img_height, img_width),
    batch_size=batch_size)

val_ds = tf.keras.utils.image_dataset_from_directory(
    root_dir,
    validation_split=0.2,
    subset="validation",
    seed=123,
    image_size=(img_height, img_width),
    batch_size=batch_size)

# Configure the dataset for performance
AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)


# 3. Get the class names
class_names = train_ds.class_names

# 4. Print the class names and the number of classes
print(f"Class names: {class_names}")
print(f"Number of classes: {len(class_names)}")

# 5. Iterate through the dataset to get a batch of images and labels
for images, labels in train_ds.take(1):
    plt.figure(figsize=(10, 10))
    # 6. Visualize a few images from the batch
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title(class_names[labels[i]])
        plt.axis("off")
    plt.show()

# 7. Calculate and print the distribution of images per class.
# This requires iterating through the full dataset, which can be time-consuming for large datasets.
# We'll do this for the training set as an example.
print("Calculating class distribution for the training set...")
class_counts = {}
for _, labels in train_ds.unbatch():
    for label in labels.numpy():
        class_name = class_names[label]
        class_counts[class_name] = class_counts.get(class_name, 0) + 1

print("\nClass distribution in training set:")
for class_name, count in class_counts.items():
    print(f"{class_name}: {count}")

"""**Reasoning**:
The previous code failed to find the main image directory within `extracted_data`. This indicates that the logic for finding the image directory is incorrect for the specific structure of the extracted files. I need to inspect the contents of the `extracted_data` directory to understand the actual directory structure and correctly set the `root_dir` for `image_dataset_from_directory`. I will use the `os` module to list the contents and determine the correct path. After identifying the correct path, I will retry loading the dataset and performing the analysis steps.


"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import os

# Inspect the extracted_data directory to find the correct image directory
extracted_dir = "extracted_data"
print("Contents of extracted_data:")
print(os.listdir(extracted_dir))

# Assuming the correct image directory is named 'PlantVillage' based on common dataset structures
# or based on the inspection of the directory contents.
# If 'PlantVillage' is not the correct directory name, you need to adjust this line
# based on the output of os.listdir(extracted_dir)
root_dir = os.path.join(extracted_dir, 'PlantVillage')

# Check if the assumed root_dir exists and contains subdirectories (classes)
if not os.path.isdir(root_dir) or not any(os.path.isdir(os.path.join(root_dir, item)) for item in os.listdir(root_dir)):
    print(f"Error: Assumed image directory '{root_dir}' not found or does not contain class subdirectories.")
    # If the assumed directory is incorrect, the next step should be to manually identify
    # the correct directory based on the printed contents of extracted_data.
else:
    print(f"Using '{root_dir}' as the root directory for image dataset loading.")

    # 2. Use image_dataset_from_directory to load the image dataset
    img_height = 224
    img_width = 224
    batch_size = 32

    train_ds = tf.keras.utils.image_dataset_from_directory(
        root_dir,
        validation_split=0.2,
        subset="training",
        seed=123,
        image_size=(img_height, img_width),
        batch_size=batch_size)

    val_ds = tf.keras.utils.image_dataset_from_directory(
        root_dir,
        validation_split=0.2,
        subset="validation",
        seed=123,
        image_size=(img_height, img_width),
        batch_size=batch_size)

    # Configure the dataset for performance
    AUTOTUNE = tf.data.AUTOTUNE
    train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

    # 3. Get the class names
    class_names = train_ds.class_names

    # 4. Print the class names and the number of classes
    print(f"Class names: {class_names}")
    print(f"Number of classes: {len(class_names)}")

    # 5. Iterate through the dataset to get a batch of images and labels
    for images, labels in train_ds.take(1):
        plt.figure(figsize=(10, 10))
        # 6. Visualize a few images from the batch
        for i in range(9):
            ax = plt.subplot(3, 3, i + 1)
            plt.imshow(images[i].numpy().astype("uint8"))
            plt.title(class_names[labels[i]])
            plt.axis("off")
        plt.show()

    # 7. Calculate and print the distribution of images per class.
    print("Calculating class distribution for the training set...")
    class_counts = {}
    for _, labels in train_ds.unbatch():
        for label in labels.numpy():
            class_name = class_names[label]
            class_counts[class_name] = class_counts.get(class_name, 0) + 1

    print("\nClass distribution in training set:")
    for class_name, count in class_counts.items():
        print(f"{class_name}: {count}")

"""## Load and analyze data

### Subtask:
Load the images and their labels from the extracted data and analyze the dataset, including visualizing some images and checking the class distribution.

**Reasoning**:
The previous attempt to load the image dataset failed because the assumed directory structure was incorrect. The `extracted_data` directory only contains a CSV file. I need to revisit the instructions, which state to check if the image data is not organized in a directory structure compatible with `image_dataset_from_directory` and print a message indicating this. I will also print the contents of the `extracted_data` directory again to confirm.
"""

import os

# Inspect the extracted_data directory again to confirm its contents
extracted_dir = "extracted_data"
print("Contents of extracted_data:")
contents = os.listdir(extracted_dir)
print(contents)

# Check if the contents indicate a directory structure suitable for image_dataset_from_directory
# This typically means subdirectories within extracted_data, not just files.
has_subdirectories = any(os.path.isdir(os.path.join(extracted_dir, item)) for item in contents)

if not has_subdirectories:
    print("\nThe extracted data does not appear to be organized in a directory structure compatible with `tf.keras.utils.image_dataset_from_directory`.")
    print("The dataset might require a different loading approach, such as parsing a CSV file to locate images.")

"""## Summary:

### Data Analysis Key Findings

*   The zip file was successfully extracted to the `extracted_data` directory.
*   The `extracted_data` directory was found to contain only a single file, `amazon.csv`, and no subdirectories containing image data.
*   The dataset structure is incompatible with the `tf.keras.utils.image_dataset_from_directory` function, which requires images to be organized into class-specific subdirectories.

### Insights or Next Steps

*   The dataset likely requires a different loading approach, such as parsing the `amazon.csv` file to obtain image paths and labels.
*   The task cannot proceed with the current approach of using `tf.keras.utils.image_dataset_from_directory` for data loading.

# Task
Analyze the image dataset provided in "/content/archive (8).zip", train a ResNet50 model on the data, and output the results in a table or image format.

## Preprocess data

### Subtask:
Preprocess the images for training the ResNet50 model, including resizing and normalization. Split the data into training and validation sets.

**Reasoning**:
The previous analysis showed that the dataset structure is not compatible with `image_dataset_from_directory`. The `amazon.csv` file likely contains the image paths and labels. The next step is to load the data from this CSV file, extract the image paths and labels, and then create TensorFlow `Dataset` objects. I will then define and apply a preprocessing function to resize and normalize the images, and finally batch and configure the datasets for performance. All these steps are necessary to prepare the data for training and can be grouped into a single code block.
"""

import pandas as pd
import tensorflow as tf
import os

# 1. Load the image paths and labels by parsing the amazon.csv file
extracted_dir = "extracted_data"
csv_path = os.path.join(extracted_dir, 'amazon.csv')

try:
    df = pd.read_csv(csv_path)
except FileNotFoundError:
    print(f"Error: {csv_path} not found.")
    # If the CSV is not found, the subtask cannot be completed.
    # The next step should be to report failure.

# Assuming the CSV has columns named 'image_path' and 'label'
# You might need to adjust column names based on the actual CSV content
# Let's inspect the dataframe first to confirm column names
print("CSV file loaded. First 5 rows:")
display(df.head())

# Assuming the column names are 'image_path' and 'label'
# If not, identify the correct column names from the df.head() output
if 'image_path' in df.columns and 'label' in df.columns:
    image_paths = df['image_path'].values
    labels = df['label'].values

    # Convert labels to numerical format if they are strings
    # Assuming labels are class names, we can create a mapping
    unique_labels = sorted(list(set(labels)))
    label_to_int = {label: i for i, label in enumerate(unique_labels)}
    numerical_labels = np.array([label_to_int[label] for label in labels])

    # 2. Create TensorFlow Dataset objects
    # Split the data into training and validation sets
    from sklearn.model_selection import train_test_split

    train_paths, val_paths, train_labels, val_labels = train_test_split(
        image_paths, numerical_labels, test_size=0.2, random_state=123, stratify=numerical_labels)

    train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))
    val_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))

    # 3. Define a preprocessing function
    img_height = 224
    img_width = 224

    def preprocess_image(image_path, label):
        # Load the image
        img = tf.io.read_file(image_path)
        img = tf.image.decode_jpeg(img, channels=3) # Assuming JPEG format
        img = tf.image.convert_image_dtype(img, tf.float32) # Convert to float [0, 1]

        # Resize the image
        img = tf.image.resize(img, [img_height, img_width])

        # Normalize the image (ResNet50 typically expects input in [-1, 1] or scaled [0, 1])
        # Here we'll normalize to [0, 1] which is done by convert_image_dtype
        # If using a model expecting [-1, 1], you'd add: img = (img * 2.0) - 1.0

        return img, label

    # 4. Apply the preprocessing function to the datasets
    train_ds = train_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
    val_ds = val_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)

    # 5. Batch the datasets
    batch_size = 32
    train_ds = train_ds.batch(batch_size)
    val_ds = val_ds.batch(batch_size)

    # 6. Configure the datasets for performance
    AUTOTUNE = tf.data.AUTOTUNE
    train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

    print("\nDatasets created and preprocessed successfully.")
    print(f"Training dataset size: {tf.data.experimental.cardinality(train_ds).numpy()} batches")
    print(f"Validation dataset size: {tf.data.experimental.cardinality(val_ds).numpy()} batches")

else:
    print("\nError: 'image_path' or 'label' column not found in the CSV. Please check the CSV file.")
    # If required columns are not found, the subtask cannot be completed.
    # The next step should be to report failure.

"""**Reasoning**:
The previous code failed to read the CSV file due to a `UnicodeDecodeError`. This indicates that the file is not encoded in UTF-8. I need to try reading the CSV with a different encoding. A common alternative is 'latin-1' or 'ISO-8859-1'. I will modify the `pd.read_csv` call to specify the encoding and retry loading the data and proceeding with the rest of the data loading and preprocessing steps as outlined in the subtask instructions.


"""

import pandas as pd
import tensorflow as tf
import numpy as np # Import numpy as it was used in the previous code block
import os
from sklearn.model_selection import train_test_split # Import train_test_split

# 1. Load the image paths and labels by parsing the amazon.csv file
extracted_dir = "extracted_data"
csv_path = os.path.join(extracted_dir, 'amazon.csv')

try:
    # Try reading with a different encoding
    df = pd.read_csv(csv_path, encoding='latin-1')
except FileNotFoundError:
    print(f"Error: {csv_path} not found.")
    # If the CSV is not found, the subtask cannot be completed.
    # The next step should be to report failure.
except UnicodeDecodeError:
    print(f"Error: Could not decode {csv_path} with 'latin-1' encoding. Try another encoding.")
    # If decoding fails with latin-1, the subtask cannot be completed with this approach.
    # The next step should be to report failure.


# Assuming the CSV has columns named 'image_path' and 'label'
# You might need to adjust column names based on the actual CSV content
# Let's inspect the dataframe first to confirm column names
print("CSV file loaded. First 5 rows:")
display(df.head())

# Assuming the column names are 'image_path' and 'label'
# If not, identify the correct column names from the df.head() output
if 'image_path' in df.columns and 'label' in df.columns:
    image_paths = df['image_path'].values
    labels = df['label'].values

    # Convert labels to numerical format if they are strings
    # Assuming labels are class names, we can create a mapping
    unique_labels = sorted(list(set(labels)))
    label_to_int = {label: i for i, label in enumerate(unique_labels)}
    numerical_labels = np.array([label_to_int[label] for label in labels])

    # 2. Create TensorFlow Dataset objects
    # Split the data into training and validation sets
    train_paths, val_paths, train_labels, val_labels = train_test_split(
        image_paths, numerical_labels, test_size=0.2, random_state=123, stratify=numerical_labels)

    train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))
    val_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))

    # 3. Define a preprocessing function
    img_height = 224
    img_width = 224

    def preprocess_image(image_path, label):
        # Load the image
        img = tf.io.read_file(image_path)
        # Handle potential errors during image decoding
        try:
            img = tf.image.decode_jpeg(img, channels=3) # Assuming JPEG format
        except tf.errors.InvalidArgumentError:
            print(f"Warning: Could not decode image at path: {image_path.numpy().decode('utf-8')}. Skipping.")
            return tf.constant([], dtype=tf.float32), tf.constant(-1, dtype=tf.int32) # Return empty tensor and a dummy label

        img = tf.image.convert_image_dtype(img, tf.float32) # Convert to float [0, 1]

        # Resize the image
        img = tf.image.resize(img, [img_height, img_width])

        # Normalize the image (ResNet50 typically expects input in [-1, 1] or scaled [0, 1])
        # Here we'll normalize to [0, 1] which is done by convert_image_dtype
        # If using a model expecting [-1, 1], you'd add: img = (img * 2.0) - 1.0

        return img, label

    # 4. Apply the preprocessing function to the datasets
    # Filter out potential bad images with dummy label -1
    train_ds = train_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE).filter(lambda img, label: label != -1)
    val_ds = val_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE).filter(lambda img, label: label != -1)


    # 5. Batch the datasets
    batch_size = 32
    train_ds = train_ds.batch(batch_size)
    val_ds = val_ds.batch(batch_size)

    # 6. Configure the datasets for performance
    AUTOTUNE = tf.data.AUTOTUNE
    train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

    print("\nDatasets created and preprocessed successfully.")
    print(f"Training dataset size: {tf.data.experimental.cardinality(train_ds).numpy()} batches")
    print(f"Validation dataset size: {tf.data.experimental.cardinality(val_ds).numpy()} batches")

else:
    print("\nError: 'image_path' or 'label' column not found in the CSV. Please check the CSV file.")
    # If required columns are not found, the subtask cannot be completed.
    # The next step should be to report failure.

"""## Summary:

### Data Analysis Key Findings

*   The `amazon.csv` file provided in the extracted data does not contain image paths and labels as expected for training an image classification model.
*   The CSV file contains tabular data with columns like 'year', 'state', 'month', 'number', and 'date', which are not suitable for identifying and labeling images.
*   Due to the absence of image files and a proper manifest, the preprocessing step for creating a TensorFlow image dataset failed.

### Insights or Next Steps

*   Verify the content of the provided zip file and ensure it contains image files and a CSV with image paths and corresponding labels.
*   If the correct data is not available, obtain a suitable image dataset for the intended image classification task.

# Task
Analyze the forest fire data in the dataset "/content/archive (8).zip" and provide the output in a table or image format.

## Inspect data

### Subtask:
Display the first few rows of the DataFrame, check the data types of each column, and look for missing values to understand the structure and quality of the data.

**Reasoning**:
Display the first few rows of the DataFrame, check the data types, and look for missing values as instructed to understand the data structure and quality.
"""

# Display the first 5 rows of the DataFrame
print("First 5 rows of the DataFrame:")
display(df.head())

# Print the data types of each column
print("\nData types of each column:")
df.info()

# Check for missing values
print("\nMissing values in each column:")
print(df.isnull().sum())

"""## Analyze trends over time

### Subtask:
Analyze the number of forest fires over the years and months to identify trends and seasonality.

**Reasoning**:
Convert the 'date' column to datetime objects, extract the year and month, and then group the data by year and month to analyze the trends and seasonality of forest fires.
"""

# 1. Convert the 'date' column to datetime objects
df['date'] = pd.to_datetime(df['date'])

# 2. Extract the year and month into separate columns
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month

# 3. Group the data by year and calculate the sum of 'number' for each year
annual_fires = df.groupby('year')['number'].sum().reset_index()

# 4. Group the data by month and calculate the sum of 'number' for each month
# Create a mapping for month numbers to names for better readability
month_name_map = {
    1: 'January', 2: 'February', 3: 'March', 4: 'April',
    5: 'May', 6: 'June', 7: 'July', 8: 'August',
    9: 'September', 10: 'October', 11: 'November', 12: 'December'
}
df['month_name'] = df['month'].map(month_name_map)
monthly_fires = df.groupby('month_name')['number'].sum().reset_index()

# Reorder months for chronological display
monthly_fires['month_name'] = pd.Categorical(monthly_fires['month_name'], categories=month_name_map.values(), ordered=True)
monthly_fires = monthly_fires.sort_values('month_name')


# 5. Display the results of the annual and monthly aggregations
print("Annual Forest Fires:")
display(annual_fires)

print("\nMonthly Forest Fires:")
display(monthly_fires)

"""## Analyze spatial distribution

### Subtask:
Analyze the distribution of forest fires across different states.

**Reasoning**:
Group the dataframe by state and sum the number of fires, then sort the results and display the state_fires dataframe.
"""

# Group by 'state' and sum the 'number' of fires
state_fires = df.groupby('state')['number'].sum().reset_index()

# Sort the results in descending order by the number of fires
state_fires = state_fires.sort_values('number', ascending=False)

# Display the state_fires DataFrame
print("Distribution of Forest Fires by State:")
display(state_fires)

"""## Analyze relationship between variables

### Subtask:
Explore relationships between different columns, such as year, month, state, and the number of fires.

**Reasoning**:
I need to calculate and display the average number of fires per month, per state, and per year to explore the relationships between these columns and the number of fires. These calculations and displays can be done in a single code block.
"""

# 1. Calculate and display the average number of fires per month
average_fires_per_month = df.groupby('month_name')['number'].mean().reset_index()

# Reorder months for chronological display
average_fires_per_month['month_name'] = pd.Categorical(average_fires_per_month['month_name'], categories=month_name_map.values(), ordered=True)
average_fires_per_month = average_fires_per_month.sort_values('month_name')


print("Average Number of Forest Fires per Month:")
display(average_fires_per_month)

# 2. Calculate and display the average number of fires per state
average_fires_per_state = df.groupby('state')['number'].mean().reset_index()
average_fires_per_state = average_fires_per_state.sort_values('number', ascending=False) # Sort for better readability

print("\nAverage Number of Forest Fires per State:")
display(average_fires_per_state)

# 3. Calculate and display the average number of fires per year
average_fires_per_year = df.groupby('year')['number'].mean().reset_index()

print("\nAverage Number of Forest Fires per Year:")
display(average_fires_per_year)

"""## Visualize data

### Subtask:
Visualize the trends and distributions of forest fires using appropriate plots.

**Reasoning**:
I need to visualize the trends and distributions of forest fires using the previously calculated dataframes: `annual_fires`, `monthly_fires`, and `state_fires`. I will create a line plot for the annual trend and bar plots for the monthly and state distributions. I will include all three plots in a single code block to accomplish the subtask efficiently.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Set a style for the plots
sns.set_style("whitegrid")

# 1. Create a line plot for annual trends
plt.figure(figsize=(12, 6))
sns.lineplot(data=annual_fires, x='year', y='number')
plt.title('Annual Trend of Forest Fires (1998-2017)')
plt.xlabel('Year')
plt.ylabel('Number of Forest Fires')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# 2. Create a bar plot for monthly distribution
plt.figure(figsize=(12, 6))
sns.barplot(data=monthly_fires, x='month_name', y='number', palette='viridis')
plt.title('Monthly Distribution of Forest Fires')
plt.xlabel('Month')
plt.ylabel('Total Number of Forest Fires')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# 3. Create a bar plot for state distribution
plt.figure(figsize=(15, 7))
sns.barplot(data=state_fires, x='state', y='number', palette='magma')
plt.title('Distribution of Forest Fires Across States')
plt.xlabel('State')
plt.ylabel('Total Number of Forest Fires')
plt.xticks(rotation=90) # Rotate labels for better readability
plt.tight_layout()
plt.show()

"""## Summary:

### Data Analysis Key Findings

*   The dataset contains 6454 entries and 5 columns: 'year', 'state', 'month', 'number', and 'date'. There are no missing values.
*   Forest fire occurrences show significant variation across the years from 1998 to 2017, without a clear monotonic trend.
*   There is a strong seasonal pattern in forest fires, with certain months experiencing significantly higher total numbers of fires (e.g., January).
*   Forest fire distribution varies considerably across states, with some states having a much higher total number of fires than others (e.g., Mato Grosso had the highest number, while Sergipe had the lowest).
*   The average number of fires also varies by month, state, and year, reinforcing the temporal and spatial variability of fire incidents.

### Insights or Next Steps

*   Investigate potential drivers behind the observed annual fluctuations and the concentration of fires in specific months and states. This could involve exploring additional data sources like climate information or land-use patterns.
*   Perform more granular analysis, such as analyzing trends within the top states or examining the relationship between specific months and states to identify high-risk periods and locations.

"""