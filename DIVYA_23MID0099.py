# -*- coding: utf-8 -*-
"""Runnable U-Net Segmentation Script (Standard Colab)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hm1BsUQkRXPwzWyvlMiwOFz7XNU7SMwH
"""

#bioWAR MODEL
#MODIFIED CODE FROM https://github.com/BioWar/Satellite-Image-Segmentation-using-Deep-Learning-for-Deforestation-Detection?tab=readme-ov-file
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation,\
                                     MaxPool2D, UpSampling2D, concatenate,\
                                     Input, Conv2DTranspose, MaxPooling2D,\
                                     Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.optimizers import Adam, RMSprop

from IPython.display import clear_output
import matplotlib.pyplot as plt
import numpy as np
import os
import random
from matplotlib import colors
from scipy.ndimage import distance_transform_edt as distance

# --- Configuration for Local Run ---
# We will use GPU/CPU strategy for local execution
strategy = tf.distribute.get_strategy()

# Global Parameters
IMG_SIZE = 512
N_CLASSES = 3  # Forest, Deforested, Other
BATCH_SIZE = 8
N_SAMPLES = 64 # Total number of synthetic samples
VALIDATION_SPLIT = 0.2
EPOCHS = 5 # Reduced epochs for a quick test run

# --- Custom Metrics (Simplified for standard compilation) ---
# We keep these for the core logic, but integrate them via model.compile

class IOU(tf.keras.metrics.Metric):
  def __init__(self, name='iou_metric', C=N_CLASSES, **kwargs):
    super(IOU, self).__init__(name=name, **kwargs)
    self.iou_total = self.add_weight(name='iou_total', initializer='zeros')
    self.C = C
    self.count = self.add_weight(name='count', initializer='zeros')

  def update_state(self, y_true, y_pred, sample_weight=None):
    # Convert predictions to hard classes (argmax)
    y_pred_argmax = tf.argmax(y_pred, axis=-1)
    y_pred_onehot = tf.one_hot(y_pred_argmax, depth=self.C)

    y_true = tf.cast(y_true, dtype=tf.float32)
    y_pred_onehot = tf.cast(y_pred_onehot, dtype=tf.float32)

    y_true_flat = tf.reshape(y_true, (-1, self.C))
    y_pred_flat = tf.reshape(y_pred_onehot, (-1, self.C))

    value = 0
    for index in range(self.C):
      intersection = tf.reduce_sum(tf.math.multiply(y_true_flat[..., index], y_pred_flat[..., index]))
      union = tf.reduce_sum(y_true_flat[..., index]) + tf.reduce_sum(y_pred_flat[..., index]) - intersection

      # Avoid division by zero
      iou_class = tf.where(union > 0, intersection / union, tf.constant(0.0))
      value += iou_class

    self.iou_total.assign_add(value / self.C)
    self.count.assign_add(1.0) # Increment the count for averaging

  def result(self):
    return self.iou_total / self.count

  def reset_states(self):
    self.iou_total.assign(0.)
    self.count.assign(0.)


class F1Score(tf.keras.metrics.Metric):
  def __init__(self, name='f1score_metric', C=N_CLASSES, **kwargs):
    super(F1Score, self).__init__(name=name, **kwargs)
    self.f1score_total = self.add_weight(name='f1score_total', initializer='zeros')
    self.C = C
    self.count = self.add_weight(name='count', initializer='zeros')

  def update_state(self, y_true, y_pred, sample_weight=None):
    # Convert predictions to hard classes (argmax)
    y_pred_argmax = tf.argmax(y_pred, axis=-1)
    y_pred_onehot = tf.one_hot(y_pred_argmax, depth=self.C)

    y_true = tf.cast(y_true, dtype=tf.float32)
    y_pred_onehot = tf.cast(y_pred_onehot, dtype=tf.float32)

    y_true_flat = tf.reshape(y_true, (-1, self.C))
    y_pred_flat = tf.reshape(y_pred_onehot, (-1, self.C))

    value = 0
    for index in range(self.C):
      TP = tf.reduce_sum(tf.math.multiply(y_pred_flat[..., index], y_true_flat[..., index]))
      Union = tf.reduce_sum(y_true_flat[..., index]) + tf.reduce_sum(y_pred_flat[..., index])

      # F1 = 2 * TP / (2 * TP + FP + FN) = 2 * TP / (Union)
      f1_class = tf.where(Union > 0, (2. * TP) / Union, tf.constant(0.0))
      value += f1_class

    self.f1score_total.assign_add(value / self.C)
    self.count.assign_add(1.0) # Increment the count for averaging

  def result(self):
    return self.f1score_total / self.count

  def reset_states(self):
    self.f1score_total.assign(0.)
    self.count.assign(0.)


# --- Custom Loss Functions (Commented out to ensure immediate runnability) ---
# The original HybridLoss and BorderLoss require complex handling (e.g., NumPy dependency in tf.function)
# and are unnecessary for verifying the model and training loop structure.

# class BorderLoss(tf.keras.losses.Loss):
#   ...

# class HybridLoss(tf.keras.losses.Loss):
#   ...


# --- Dataset Creation with Synthetic Data ---

def create_synthetic_data(num_samples, img_size, n_classes):
    """Generates synthetic image and mask data for testing."""
    print(f"[INFO] Generating {num_samples} synthetic data samples...")
    # Generate random images (normalized)
    images = np.random.rand(num_samples, img_size, img_size, 3).astype(np.float32)

    # Generate random integer masks (class indices)
    mask_indices = np.random.randint(0, n_classes, size=(num_samples, img_size, img_size))

    # Convert mask indices to one-hot encoding (N, 512, 512, 3)
    masks = tf.keras.utils.to_categorical(mask_indices, num_classes=n_classes)

    # Split data
    split_idx = int(num_samples * (1 - VALIDATION_SPLIT))

    X_train = images[:split_idx]
    y_train = masks[:split_idx]
    X_val = images[split_idx:]
    y_val = masks[split_idx:]

    print(f"Synthetic Data Shapes: Train Images: {X_train.shape}, Train Masks: {y_train.shape}")
    return X_train, y_train, X_val, y_val

X_train, y_train, X_val, y_val = create_synthetic_data(N_SAMPLES, IMG_SIZE, N_CLASSES)

# Use TensorFlow Data pipeline for training
train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(2048).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
validation_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

# Get a sample image/mask for prediction visualization
for img, msk in validation_dataset.take(1):
    sample_image = img[0]
    sample_mask = msk[0]
    break

# Display a sample (this will look random since the data is synthetic)
print("\n[INFO] Displaying one synthetic sample (Image and One-Hot Mask):")
plt.figure(figsize=(8, 4))
plt.subplot(1, 2, 1)
plt.title('Sample Image (Synthetic)')
plt.imshow(sample_image.numpy())
plt.axis('off')

plt.subplot(1, 2, 2)
# To display the one-hot mask, we convert it back to index form
plt.title('Sample Mask (One-Hot Encoded)')
plt.imshow(tf.argmax(sample_mask, axis=-1).numpy(), cmap='viridis')
plt.axis('off')
plt.show()

# --- Model Building Functions (Original code kept) ---

def conv2d_block(input_tensor, n_filters, kernel_size=3, batchnorm=True, sublayers=2):
    for idx in range(sublayers):
        conv = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size),
                      kernel_initializer="he_normal", padding="same")(input_tensor if idx==0 else conv)
        if batchnorm:
            normalized = BatchNormalization()(conv)
        conv = Activation("relu")(normalized if batchnorm else conv)
    return conv

def conv2d_transpose_block(input_tensor, concatenate_tensor, n_filters, kernel_size=3, strides=2, transpose=False):
    if transpose:
        conv = Conv2DTranspose(n_filters, (kernel_size, kernel_size),
                               strides=(strides, strides), padding='same')(input_tensor)
    else:
        conv = Conv2D(n_filters, (kernel_size, kernel_size), activation = 'relu', padding = 'same',
                      kernel_initializer = 'he_normal')(UpSampling2D(size=(kernel_size, kernel_size))(input_tensor))
    conv = Activation("relu")(conv)
    concatenation = concatenate([conv, concatenate_tensor])
    return concatenation

def make_unet_experimental(input_shape=(IMG_SIZE, IMG_SIZE, 3)):
    # This function is derived from the complex one provided in the original code.
    inputs = Input(input_shape)

    # --- ENCODER ---
    conv1 = Conv2D(16, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)
    conv1 = Conv2D(16, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)

    conv2 = Conv2D(32, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)
    conv2 = Conv2D(32, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)
    up2_1 = Conv2D(32, 3, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv2))
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)

    conv3 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)
    conv3 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)
    up3_1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv3))
    up3_2 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(up3_1))
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)

    conv4 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)
    conv4 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)
    up4_1 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv4))
    up4_2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(up4_1))
    up4_3 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(up4_2))
    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)

    # --- BOTTLENECK ---
    conv5 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)
    conv5 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)
    up5_1 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv5))
    up5_2 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(up5_1))
    up5_3 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(up5_2))
    up5_4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(up5_3))

    # --- DECODER ---
    up6 = Conv2D(128, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv5))
    merge6 = concatenate([conv4, up6, up5_1], axis=3)
    conv6 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge6)
    conv6 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)

    up7 = Conv2D(64, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv6))
    merge7 = concatenate([conv3, up7, up5_2, up4_1], axis=3)
    conv7 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge7)
    conv7 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)

    up8 = Conv2D(32, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv7))
    merge8 = concatenate([conv2, up8, up5_3, up4_2, up3_1], axis=3)
    conv8 = Conv2D(32, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge8)
    conv8 = Conv2D(32, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)

    up9 = Conv2D(16, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv8))
    merge9 = concatenate([conv1, up9, up5_4, up4_3, up3_2, up2_1], axis=3)
    conv9 = Conv2D(16, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge9)
    conv9 = Conv2D(16, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)

    # --- OUTPUT ---
    conv10 = Conv2D(N_CLASSES, 1, activation='softmax')(conv9)
    model = Model(inputs=inputs, outputs=conv10)
    return model

# --- Model Compilation and Training ---

with strategy.scope():
    # Build model using the complex experimental structure
    model = make_unet_experimental()
    print("\n[INFO] U-Net Experimental Model Summary:")
    model.summary()

    # Optimizer and Learning Rate (from original code)
    learning_rate = 1e-5
    optimizer = RMSprop(learning_rate=learning_rate, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False)

    # Compile the model with standard loss and the custom metrics
    model.compile(
        optimizer=optimizer,
        loss=CategoricalCrossentropy(), # Using standard Keras loss for robustness
        metrics=['accuracy', IOU(name='test_IoU'), F1Score(name='test_f1score')]
    )

print(f"\n[INFO] Starting simplified training for {EPOCHS} epochs (using synthetic data)...")

# Use standard Keras model.fit, which handles the training loop internally
history = model.fit(
    train_dataset,
    epochs=EPOCHS,
    validation_data=validation_dataset,
    verbose=1 # Show progress
)

# --- Model Evaluation and Visualization ---

def show_predictions():
    """Predicts and displays the result for the single sample image."""
    prediction = model.predict(sample_image[tf.newaxis, ...], verbose=0)[0]

    plt.figure(figsize=(15, 5))

    title = ['Input Image', 'True Mask (Indices)', 'Predicted Mask (Indices)']

    # True mask: convert one-hot to class indices for display
    true_mask_indices = tf.argmax(sample_mask, axis=-1).numpy()

    # Predicted mask: convert prediction probabilities to class indices
    pred_mask_indices = tf.argmax(prediction, axis=-1).numpy()

    display_list = [sample_image.numpy(), true_mask_indices, pred_mask_indices]

    for i in range(len(display_list)):
        plt.subplot(1, len(display_list), i+1)
        plt.title(title[i])

        # Images should be displayed as normal
        if i == 0:
            plt.imshow(display_list[i])
        # Masks should use a color map to show classes
        else:
            plt.imshow(display_list[i], cmap='viridis', interpolation='nearest')
        plt.axis('off')
    plt.suptitle("Prediction Example (Synthetic Data)")
    plt.show()

print("\n[INFO] Training complete. Showing final prediction on sample:")
show_predictions()


model_save_path = "/content/local_unet_model.h5"
model.save(model_save_path)
print(f"\n[INFO] Model saved locally at: {model_save_path}")

